{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":81878,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68806,"modelId":91102}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install flask\n!pip install pyngrok\n!pip install langchain_community","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-16T06:34:42.359766Z","iopub.execute_input":"2024-08-16T06:34:42.360194Z","iopub.status.idle":"2024-08-16T06:35:33.348526Z","shell.execute_reply.started":"2024-08-16T06:34:42.360162Z","shell.execute_reply":"2024-08-16T06:35:33.347346Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: flask in /opt/conda/lib/python3.10/site-packages (3.0.3)\nRequirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from flask) (3.0.3)\nRequirement already satisfied: Jinja2>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from flask) (3.1.2)\nRequirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from flask) (2.2.0)\nRequirement already satisfied: click>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from flask) (8.1.7)\nRequirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from flask) (1.8.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.1.2->flask) (2.1.3)\nCollecting pyngrok\n  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.10/site-packages (from pyngrok) (6.0.1)\nDownloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\nInstalling collected packages: pyngrok\nSuccessfully installed pyngrok-7.2.0\nCollecting langchain_community\n  Downloading langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (3.9.1)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.6.7)\nCollecting langchain<0.3.0,>=0.2.13 (from langchain_community)\n  Downloading langchain-0.2.14-py3-none-any.whl.metadata (7.1 kB)\nCollecting langchain-core<0.3.0,>=0.2.30 (from langchain_community)\n  Downloading langchain_core-0.2.32-py3-none-any.whl.metadata (6.2 kB)\nCollecting langsmith<0.2.0,>=0.1.0 (from langchain_community)\n  Downloading langsmith-0.1.99-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (1.26.4)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.13->langchain_community)\n  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.13->langchain_community) (2.5.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.30->langchain_community) (1.33)\nCollecting packaging<25,>=23.2 (from langchain-core<0.3.0,>=0.2.30->langchain_community)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.30->langchain_community) (4.9.0)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain_community)\n  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2024.7.4)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain_community) (2.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain_community) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain_community) (2.14.6)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\nDownloading langchain_community-0.2.12-py3-none-any.whl (2.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain-0.2.14-py3-none-any.whl (997 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.2.32-py3-none-any.whl (389 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.8/389.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langsmith-0.1.99-py3-none-any.whl (140 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\nDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, orjson, langsmith, langchain-core, langchain-text-splitters, langchain, langchain_community\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.6.1 requires cubinlinker, which is not installed.\ncudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires ptxcompiler, which is not installed.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\ncudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.5.1 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.2.14 langchain-core-0.2.32 langchain-text-splitters-0.2.2 langchain_community-0.2.12 langsmith-0.1.99 orjson-3.10.7 packaging-24.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!ngrok config add-authtoken 2kNdZgjsPXcfBbK7YkZfKWLuQCL_4uQWmw1UV7qyJgPf3YxLW","metadata":{"execution":{"iopub.status.busy":"2024-08-16T06:35:33.351341Z","iopub.execute_input":"2024-08-16T06:35:33.352247Z","iopub.status.idle":"2024-08-16T06:35:35.252600Z","shell.execute_reply.started":"2024-08-16T06:35:33.352198Z","shell.execute_reply":"2024-08-16T06:35:35.251477Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport sqlite3\nfrom flask import Flask, request, jsonify\nimport requests\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.pool import StaticPool\nimport sqlparse\nfrom langchain_community.utilities.sql_database import SQLDatabase","metadata":{"execution":{"iopub.status.busy":"2024-08-16T06:35:35.254779Z","iopub.execute_input":"2024-08-16T06:35:35.255292Z","iopub.status.idle":"2024-08-16T06:35:40.901683Z","shell.execute_reply.started":"2024-08-16T06:35:35.255235Z","shell.execute_reply":"2024-08-16T06:35:40.900631Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Access the Hugging Face token from Kaggle secrets\nhuggingface_token = \"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\"\n# Log in to Hugging Face\nlogin(huggingface_token)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T06:35:40.904825Z","iopub.execute_input":"2024-08-16T06:35:40.907018Z","iopub.status.idle":"2024-08-16T06:35:41.143608Z","shell.execute_reply.started":"2024-08-16T06:35:40.906982Z","shell.execute_reply":"2024-08-16T06:35:41.142516Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2024-08-16T06:35:41.145043Z","iopub.execute_input":"2024-08-16T06:35:41.145483Z","iopub.status.idle":"2024-08-16T06:35:41.183208Z","shell.execute_reply.started":"2024-08-16T06:35:41.145446Z","shell.execute_reply":"2024-08-16T06:35:41.181641Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"available_memory = torch.cuda.get_device_properties(0).total_memory\nprint(available_memory)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T06:35:41.184680Z","iopub.execute_input":"2024-08-16T06:35:41.185196Z","iopub.status.idle":"2024-08-16T06:35:41.220702Z","shell.execute_reply.started":"2024-08-16T06:35:41.185143Z","shell.execute_reply":"2024-08-16T06:35:41.219559Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"17059545088\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = \"defog/sqlcoder-7b-2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif available_memory > 15e9:\n    # if you have atleast 15GB of GPU memory, run load the model in float16\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        trust_remote_code=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        use_cache=True,\n    )\nelse:\n    # else, load in 8 bits – this is a bit slower\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        trust_remote_code=True,\n        # torch_dtype=torch.float16,\n        load_in_8bit=True,\n        device_map=\"auto\",\n        use_cache=True,\n    )","metadata":{"execution":{"iopub.status.busy":"2024-08-16T06:35:41.222607Z","iopub.execute_input":"2024-08-16T06:35:41.223032Z","iopub.status.idle":"2024-08-16T06:37:03.958265Z","shell.execute_reply.started":"2024-08-16T06:35:41.222994Z","shell.execute_reply":"2024-08-16T06:37:03.956966Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4230df88e0124b66ae5c4d0236ddcdcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4830d4cde5640858371f872f3366567"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abc460e7077a46bd80e896436a2a112e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/515 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2264ee34a8d48608f76c8a1a454b70e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/691 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b61abe68a83444089b7782a4f2f07290"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3be2b08c9ae4538943759731f1e0d70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bc7cb3f0c56485e941f4d1bd5ef2f00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cf80ee3a89b4ea6ba0d549ca9786f18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b6729d8605b42ffa1653f833f000903"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98c8b39f1f304c878c52d41e902f165e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7bd73ea348a49b99a16e07afd654e0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dd9019eebde4f58af11be245b0ee2c9"}},"metadata":{}}]},{"cell_type":"code","source":"def get_engine_for_chinook_db():\n    \"\"\"Pull sql file, populate in-memory database, and create engine.\"\"\"\n    url = \"https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql\"\n    response = requests.get(url)\n    sql_script = response.text\n\n    connection = sqlite3.connect(\":memory:\", check_same_thread=False)\n    connection.executescript(sql_script)\n    return create_engine(\n        \"sqlite://\",\n        creator=lambda: connection,\n        poolclass=StaticPool,\n        connect_args={\"check_same_thread\": False},\n    )","metadata":{"execution":{"iopub.status.busy":"2024-08-16T06:37:03.959871Z","iopub.execute_input":"2024-08-16T06:37:03.960447Z","iopub.status.idle":"2024-08-16T06:37:03.970259Z","shell.execute_reply.started":"2024-08-16T06:37:03.960414Z","shell.execute_reply":"2024-08-16T06:37:03.967160Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"prompt = \"\"\"### Task\n            Generate a SQL query to answer [QUESTION]{question}[/QUESTION]\n\n            ### Instructions\n            - If you cannot answer the question with the available database schema, return 'I do not know'\n            - Remember that revenue is price multiplied by quantity\n            - Remember that cost is supply_price multiplied by quantity\n            - Make sure you return the Query based on the SQLite\n\n            ### Database Schema\n            This query will run on a database whose schema is represented in this string:\n            {schema}\n\n            ### Answer\n            Given the database schema, here is the SQL query that answers [QUESTION]{question}[/QUESTION]\n            [SQL]\n        \"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-08-16T06:51:14.737883Z","iopub.execute_input":"2024-08-16T06:51:14.738977Z","iopub.status.idle":"2024-08-16T06:51:14.744500Z","shell.execute_reply.started":"2024-08-16T06:51:14.738937Z","shell.execute_reply":"2024-08-16T06:51:14.743289Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def generate_query(question , schema):\n    updated_prompt = prompt.format(question=question , schema = schema)\n    inputs = tokenizer(updated_prompt, return_tensors=\"pt\").to(\"cuda\")\n    generated_ids = model.generate(\n        **inputs,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.eos_token_id,\n        max_new_tokens=400,\n        do_sample=False,\n        num_beams=1,\n    )\n    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    # empty cache so that you do generate more results w/o memory crashing\n    # particularly important on Colab – memory management is much more straightforward\n    # when running on an inference service\n    return sqlparse.format(outputs[0].split(\"[SQL]\")[-1], reindent=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T06:51:18.132493Z","iopub.execute_input":"2024-08-16T06:51:18.132917Z","iopub.status.idle":"2024-08-16T06:51:18.140897Z","shell.execute_reply.started":"2024-08-16T06:51:18.132886Z","shell.execute_reply":"2024-08-16T06:51:18.139732Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# engine = get_engine_for_chinook_db()\n# db = SQLDatabase(engine)\n# schema = db.table_info","metadata":{"execution":{"iopub.status.busy":"2024-08-16T06:51:23.639137Z","iopub.execute_input":"2024-08-16T06:51:23.639561Z","iopub.status.idle":"2024-08-16T06:51:23.644592Z","shell.execute_reply.started":"2024-08-16T06:51:23.639531Z","shell.execute_reply":"2024-08-16T06:51:23.643478Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# APP","metadata":{}},{"cell_type":"code","source":"from pyngrok import ngrok\n\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef home():\n    return \"Welcome to SQL Query Generator!\"\n\n\n@app.route('/generate-sql', methods=['POST'])\ndef generate_sql():\n    \"\"\"API endpoint to generate SQL query.\"\"\"\n    data = request.json\n    question = data.get('question')\n    schema   = data.get('schema')\n    \n\n    if not question or not schema:\n        return jsonify({'error': 'Question and schema are required'}), 400\n\n    try:\n        sql_query = generate_query(question, schema)\n        return jsonify({'sql_query': sql_query})\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n\nif __name__ == '__main__':\n#     app.run(host='0.0.0.0', port=5000)\n    public_url = ngrok.connect(5000)\n    print(f\" * ngrok tunnel available at {public_url}\")\n    app.run(debug=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T06:51:24.590712Z","iopub.execute_input":"2024-08-16T06:51:24.591124Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":" * ngrok tunnel available at NgrokTunnel: \"https://9be9-34-75-214-143.ngrok-free.app\" -> \"http://localhost:5000\"\n * Serving Flask app '__main__'\n * Debug mode: off\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"# !pip install langchain_openai langchain_community langchain pymysql chromadb -q","metadata":{"execution":{"iopub.status.busy":"2024-08-16T06:51:10.789277Z","iopub.execute_input":"2024-08-16T06:51:10.789938Z","iopub.status.idle":"2024-08-16T06:51:10.795544Z","shell.execute_reply.started":"2024-08-16T06:51:10.789902Z","shell.execute_reply":"2024-08-16T06:51:10.794355Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# ! pip install langchain_huggingface","metadata":{"execution":{"iopub.status.busy":"2024-08-16T06:51:10.796839Z","iopub.execute_input":"2024-08-16T06:51:10.797206Z","iopub.status.idle":"2024-08-16T06:51:10.808922Z","shell.execute_reply.started":"2024-08-16T06:51:10.797173Z","shell.execute_reply":"2024-08-16T06:51:10.807881Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# from langchain_huggingface.llms import HuggingFacePipeline\n# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# # model_id = \"defog/sqlcoder-7b-2\"\n# # tokenizer = AutoTokenizer.from_pretrained(model_id)\n# # model = AutoModelForCausalLM.from_pretrained(model_id).to('cuda')  # Move model to GPU\n\n# pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256)  # device=0 refers to GPU 0\n# hf = HuggingFacePipeline(pipeline=pipe)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-16T06:51:10.810576Z","iopub.execute_input":"2024-08-16T06:51:10.810935Z","iopub.status.idle":"2024-08-16T06:51:10.821610Z","shell.execute_reply.started":"2024-08-16T06:51:10.810907Z","shell.execute_reply":"2024-08-16T06:51:10.820553Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# from langchain_core.prompts import PromptTemplate\n\n# template = \"\"\"Question: {question}\n\n# Answer: Let's think step by step.\"\"\"\n# prompt = PromptTemplate.from_template(template)\n\n# chain = prompt | hf\n\n# question = \"How many employee in employe tables\"\n\n# print(chain.invoke({\"question\": question}))","metadata":{"execution":{"iopub.status.busy":"2024-08-16T06:51:10.822735Z","iopub.execute_input":"2024-08-16T06:51:10.823086Z","iopub.status.idle":"2024-08-16T06:51:10.836664Z","shell.execute_reply.started":"2024-08-16T06:51:10.823034Z","shell.execute_reply":"2024-08-16T06:51:10.835677Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}